{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8809152f",
   "metadata": {},
   "source": [
    "# Low Rank Adaptation (LoRA) Training for FLUX.1-dev Model\n",
    "\n",
    "This notebook trains a LoRA adapter specifically for the **FLUX.1-dev** model by Black Forest Labs.\n",
    "\n",
    "## Key Differences from Stable Diffusion LoRA:\n",
    "\n",
    "### Architecture Changes:\n",
    "- **Base Model**: `black-forest-labs/FLUX.1-dev` (instead of Stable Diffusion 1.5)\n",
    "- **Text Encoder**: T5EncoderModel (instead of CLIP)\n",
    "- **Main Model**: FluxTransformer2DModel (instead of UNet2DConditionModel)\n",
    "- **Scheduler**: FlowMatchEulerDiscreteScheduler (instead of DDPM)\n",
    "- **Pipeline**: FluxPipeline (text2img, no img2img)\n",
    "\n",
    "### Optimizations for RTX4090:\n",
    "- **Resolution**: 1024x1024 (instead of 512x512)\n",
    "- **Batch Size**: 1 (FLUX requires more VRAM)\n",
    "- **Mixed Precision**: bf16 (optimal for FLUX)\n",
    "- **LoRA Rank**: 16 (higher for better quality)\n",
    "- **LoRA Alpha**: 32\n",
    "\n",
    "### Generation Parameters:\n",
    "- **Guidance Scale**: 3.5 (FLUX works better with lower guidance)\n",
    "- **Steps**: 28 (optimal for FLUX)\n",
    "- **Output**: Direct text2img generation\n",
    "\n",
    "⚠️ **Requirements**: This notebook requires significant VRAM (8GB+ recommended) and the FLUX.1-dev model access on HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b0106",
   "metadata": {},
   "source": [
    "## Imports + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install compatible versions - balance between too new and too old\n",
    "%pip install pillow pillow-avif-plugin openai \"diffusers==0.30.2\" \"huggingface_hub==0.24.6\" accelerate peft datasets dotenv rich tqdm ipywidgets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4465d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.logging import RichHandler\n",
    "import logging\n",
    "\n",
    "console = Console()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=\"INFO\",\n",
    "    format=\"%(message)s\",\n",
    "    datefmt=\"[%X]\",\n",
    "    handlers=[RichHandler(console=console)]\n",
    ")\n",
    "log = logging.getLogger(\"rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97059c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, os, re, shutil\n",
    "\n",
    "IMAGES_OLD_DIR = Path(\"./images-old\")\n",
    "IMAGES_NEW_DIR = Path(\"./images-new\")\n",
    "\n",
    "WORK_DIR = Path(\"./work\")\n",
    "CONVERTED_DIR = WORK_DIR / \"converted\"\n",
    "CONVERTED_OLD = CONVERTED_DIR / \"old\"\n",
    "CONVERTED_NEW = CONVERTED_DIR / \"new\"\n",
    "LABELS_DIR = WORK_DIR / \"labels\"\n",
    "DATASET_DIR = WORK_DIR / \"dataset\"\n",
    "LORA_OUTPUT_DIR = WORK_DIR / \"lora-weights\"\n",
    "\n",
    "for p in [WORK_DIR, CONVERTED_OLD, CONVERTED_NEW, LABELS_DIR, DATASET_DIR, LORA_OUTPUT_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET_EXT = \".jpg\"\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY_HERE\")\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "LABEL_SCHEMA = [\n",
    "    \"perspective (e.g., top-down, eye-level, low-angle)\",\n",
    "    \"background (describe setting, simplicity vs clutter)\",\n",
    "    \"lighting (e.g., soft, harsh, high-key, low-key, golden-hour)\",\n",
    "    \"color palette (e.g., muted, vibrant, pastel, monochrome)\",\n",
    "    \"texture treatment (e.g., painterly, grainy, smooth)\",\n",
    "    \"composition (e.g., centered subject, rule of thirds, negative space)\",\n",
    "    \"mood (e.g., nostalgic, cheerful, moody, futuristic)\",\n",
    "]\n",
    "\n",
    "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
    "USE_8BIT_ADAM = True\n",
    "MIXED_PRECISION = \"bf16\"  # FLUX works better with bfloat16\n",
    "SEED = 42\n",
    "\n",
    "log.info(\"Settings loaded.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce2141",
   "metadata": {},
   "source": [
    "## Convert images to JPGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569239b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def convert_folder(src: Path, dst: Path, target_ext: str = \".jpg\"):\n",
    "    count = 0\n",
    "    for p in tqdm(sorted(src.rglob(\"*\"))):\n",
    "        if p.is_dir() or p.name.startswith(\".\"):\n",
    "            continue\n",
    "        if p.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\", \".avif\", \".webp\"]:\n",
    "            # skip non-image files explicitly\n",
    "            continue\n",
    "        try:\n",
    "            with Image.open(p) as im:\n",
    "                im = im.convert(\"RGB\")  # unify colorspace\n",
    "                out_path = dst / (p.stem + target_ext)\n",
    "                out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                if target_ext.lower() == \".jpg\":                    \n",
    "                    im.save(out_path, format=\"JPEG\", quality=95, optimize=True)\n",
    "                else:\n",
    "                    im.save(out_path)\n",
    "                count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to convert {p}: {e}\")\n",
    "    return count\n",
    "\n",
    "n_old = convert_folder(IMAGES_OLD_DIR, CONVERTED_OLD, TARGET_EXT)\n",
    "n_new = convert_folder(IMAGES_NEW_DIR, CONVERTED_NEW, TARGET_EXT)\n",
    "log.info(f\"Converted {n_old} old images and {n_new} new images to {TARGET_EXT}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0362992b",
   "metadata": {},
   "source": [
    "# Image labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac808dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import time\n",
    "import base64\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")\n",
    "\n",
    "LABEL_SCHEMA = [\n",
    "    \"food item (main dish, ingredient, or drink that appears)\",\n",
    "    \"perspective (e.g., top-down, eye-level, low-angle)\",\n",
    "    \"background (describe setting, simplicity vs clutter)\",\n",
    "    \"lighting (e.g., soft, harsh, high-key, low-key, golden-hour)\",\n",
    "    \"color palette (e.g., muted, vibrant, pastel, monochrome)\",\n",
    "    \"texture treatment (e.g., painterly, grainy, smooth)\",\n",
    "    \"composition (e.g., centered subject, rule of thirds, negative space)\",\n",
    "    \"mood (e.g., nostalgic, cheerful, moody, futuristic)\",\n",
    "]\n",
    "\n",
    "\n",
    "def b64_image(image_path: Path) -> str:\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "def load_existing_labels(jsonl_path: Path) -> set:\n",
    "    \"\"\"Load already labeled filenames from existing JSONL file\"\"\"\n",
    "    labeled_files = set()\n",
    "    if jsonl_path.exists():\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    if \"filename\" in data:\n",
    "                        labeled_files.add(str(Path(data[\"filename\"]).resolve()))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return labeled_files\n",
    "\n",
    "def label_image(img_path: Path, schema):\n",
    "    img_b64 = b64_image(img_path)\n",
    "    schema_bullets = \"\\n\".join([f\"- {s}\" for s in schema])\n",
    "    system_prompt = (\n",
    "        \"You are a meticulous food and style image annotator. \"\n",
    "        \"Return a strict JSON object with fields exactly matching the requested schema.\"\n",
    "    )\n",
    "    user_prompt = f\"\"\"\n",
    "Analyze the image and label it for the following attributes:\n",
    "{schema_bullets}\n",
    "\n",
    "Rules:\n",
    "- Return strictly valid JSON (no markdown).\n",
    "- For 'food item', use the most specific name possible (e.g., 'spaghetti carbonara', 'latte art', 'sushi roll').\n",
    "- For style attributes, use short descriptive phrases.\n",
    "- Include a 'notes' field for anything noteworthy.\n",
    "\"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=AZURE_DEPLOYMENT_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": user_prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n",
    "                ]},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        text = resp.choices[0].message.content\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception:\n",
    "            m = re.search(r\"\\{[\\s\\S]*\\}\", text)\n",
    "            log.error(f\"⚠️ Non-JSON response for {img_path}:\\n{text}\")\n",
    "            return json.loads(m.group(0)) if m else {\"raw\": text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def label_folder(src: Path, out_jsonl: Path, max_images: int = 10, override: bool = False):\n",
    "    n = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    # Load existing labels if not overriding\n",
    "    labeled_files = set() if override else load_existing_labels(out_jsonl)\n",
    "    \n",
    "    # Open file in append mode if not overriding and file exists\n",
    "    mode = \"w\" if override or not out_jsonl.exists() else \"a\"\n",
    "    \n",
    "    with open(out_jsonl, mode, encoding=\"utf-8\") as out:\n",
    "        for p in tqdm(sorted(src.glob(f\"*{TARGET_EXT}\"))):\n",
    "            if n >= max_images:\n",
    "                print(f\"⏹️ Stopping after {max_images} images\")\n",
    "                break\n",
    "                \n",
    "            # Check if already labeled\n",
    "            if str(p.resolve()) in labeled_files:\n",
    "                log.info(f\"⏭️ Skipping {p.name} (already labeled)\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "                \n",
    "            log.info(f\"Labeling {p}...\")\n",
    "            result = label_image(p, LABEL_SCHEMA)\n",
    "            rec = {\"filename\": str(p.resolve()), \"labels\": result}\n",
    "            out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            n += 1\n",
    "            time.sleep(0.2)            \n",
    "    \n",
    "    print(f\"✅ Labeled {n} new images, skipped {skipped} already labeled → {out_jsonl}\")\n",
    "\n",
    "print(\"Labeling images via Azure OpenAI...\")\n",
    "label_folder(CONVERTED_NEW, LABELS_DIR / \"new_labels.jsonl\", max_images=50, override=False)\n",
    "label_folder(CONVERTED_OLD, LABELS_DIR / \"old_labels.jsonl\", max_images=50, override=False)\n",
    "print(\"🎉 Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c2d83",
   "metadata": {},
   "source": [
    "# Image captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def captions_from_labels(jsonl_path: Path, out_dir: Path) -> int:\n",
    "    \"\"\"\n",
    "    Reads a JSONL file with Azure OpenAI labeling results and builds\n",
    "    caption .txt files next to each image for LoRA training.\n",
    "\n",
    "    Captions include the food item first, followed by style attributes.\n",
    "    Uses tqdm for progress and rich logging for clean notebook output.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dst_img_dir = out_dir / \"new\"\n",
    "    dst_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # Count lines first for tqdm total\n",
    "    total = sum(1 for _ in open(jsonl_path, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "    log.info(f\"📑 Building captions from {jsonl_path} ({total} records)...\")\n",
    "\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, total=total, desc=\"Generating captions\", leave=True):\n",
    "            rec = json.loads(line)\n",
    "            fp = Path(rec[\"filename\"])\n",
    "            labels = rec.get(\"labels\", {})\n",
    "\n",
    "            food = labels.get(\"food item\") or labels.get(\"food item (main dish, ingredient, or drink that appears)\")\n",
    "            perspective = labels.get(\"perspective\")\n",
    "            background = labels.get(\"background\")\n",
    "            lighting = labels.get(\"lighting\")\n",
    "            palette = labels.get(\"color palette\")\n",
    "            texture = labels.get(\"texture treatment\")\n",
    "            composition = labels.get(\"composition\")\n",
    "            mood = labels.get(\"mood\")\n",
    "\n",
    "            parts = []\n",
    "            if food:\n",
    "                parts.append(f\"a photo of {food}\")\n",
    "            if composition:\n",
    "                parts.append(f\"composition: {composition}\")\n",
    "            if perspective:\n",
    "                parts.append(f\"perspective: {perspective}\")\n",
    "            if lighting:\n",
    "                parts.append(f\"lighting: {lighting}\")\n",
    "            if palette:\n",
    "                parts.append(f\"color palette: {palette}\")\n",
    "            if texture:\n",
    "                parts.append(f\"texture: {texture}\")\n",
    "            if background:\n",
    "                parts.append(f\"background: {background}\")\n",
    "            if mood:\n",
    "                parts.append(f\"mood: {mood}\")\n",
    "\n",
    "            caption = \", \".join(parts) + \".\"\n",
    "            if not caption.strip() or caption == \".\":\n",
    "                caption = \"A food scene with distinctive style.\"\n",
    "\n",
    "            out_img_path = dst_img_dir / fp.name\n",
    "            out_txt_path = dst_img_dir / (fp.stem + \".txt\")\n",
    "\n",
    "            try:\n",
    "                shutil.copy2(fp, out_img_path)\n",
    "                with open(out_txt_path, \"w\", encoding=\"utf-8\") as out:\n",
    "                    out.write(caption)\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                log.error(f\"⚠️ Failed to process {fp.name}: {e}\")\n",
    "\n",
    "    log.info(f\"✅ Wrote captions for {count} images into {dst_img_dir}\")\n",
    "    return count\n",
    "\n",
    "n1 = captions_from_labels(LABELS_DIR / \"new_labels.jsonl\", DATASET_DIR)\n",
    "n2 = captions_from_labels(LABELS_DIR / \"old_labels.jsonl\", DATASET_DIR)\n",
    "log.info(f\"🎉 Done. Total captions created: {n1 + n2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e43419",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch version and fix if needed\n",
    "import torch\n",
    "print(f\"Current PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if we have a proper PyTorch installation\n",
    "try:\n",
    "    # Test basic PyTorch functionality\n",
    "    x = torch.tensor([1.0, 2.0])\n",
    "    print(f\"PyTorch tensor test: {x}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "except Exception as e:\n",
    "    print(f\"PyTorch test failed: {e}\")\n",
    "\n",
    "# The issue might be that the development version isn't recognized by transformers\n",
    "# Let's patch the version check by setting a compatible version string\n",
    "import sys\n",
    "if hasattr(torch, '__version__'):\n",
    "    original_version = torch.__version__\n",
    "    if 'a0' in torch.__version__:\n",
    "        print(f\"Detected development PyTorch version: {original_version}\")\n",
    "        print(\"Patching version string for compatibility...\")\n",
    "        # Set a stable version string that transformers will accept\n",
    "        torch.__version__ = \"2.1.0\"\n",
    "        print(f\"Patched PyTorch version to: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f165aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, torch, numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import T5EncoderModel, T5TokenizerFast\n",
    "from diffusers import FluxTransformer2DModel, AutoencoderKL, FlowMatchEulerDiscreteScheduler\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import snapshot_download\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# --- Reproducibility ---\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# --- Download model repo locally (only first run actually downloads) ---\n",
    "repo_id = \"black-forest-labs/FLUX.1-dev\"\n",
    "local_dir = \"./flux-dev-model\"\n",
    "snapshot_download(repo_id=repo_id, local_dir=local_dir)\n",
    "BASE_MODEL_ID = local_dir  # ensure we use the local copy\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "TRAIN_BATCH_SIZE = 1      # FLUX requires more memory\n",
    "LR = 1e-4\n",
    "MAX_STEPS = 5000\n",
    "IMG_RES = 1024\n",
    "LORA_R = 16               # Higher rank for better quality\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "log.info(\"Loading FLUX model components...\")\n",
    "\n",
    "# --- Load tokenizer & encoder (T5 only for FLUX) ---\n",
    "tokenizer = T5TokenizerFast.from_pretrained(BASE_MODEL_ID, subfolder=\"tokenizer_2\")\n",
    "text_encoder = T5EncoderModel.from_pretrained(BASE_MODEL_ID, subfolder=\"text_encoder_2\")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(BASE_MODEL_ID, subfolder=\"vae\")\n",
    "transformer = FluxTransformer2DModel.from_pretrained(BASE_MODEL_ID, subfolder=\"transformer\")\n",
    "\n",
    "# --- Apply LoRA to transformer ---\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"FEATURE_EXTRACTION\",\n",
    ")\n",
    "transformer = get_peft_model(transformer, lora_config)\n",
    "log.info(\"LoRA applied to FLUX Transformer\")\n",
    "\n",
    "# --- Dataset ---\n",
    "class CaptionImageDataset(Dataset):\n",
    "    def __init__(self, img_dir: Path, tokenizer, size=1024):\n",
    "        self.paths = sorted(img_dir.glob(\"*.jpg\"))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        log.info(f\"Dataset created with {len(self.paths)} images\")\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        caption = img_path.with_suffix(\".txt\").read_text(encoding=\"utf-8\").strip()\n",
    "\n",
    "        # Preprocess image → [-1, 1] tensor\n",
    "        image = Image.open(img_path).convert(\"RGB\").resize((self.size, self.size), Image.BICUBIC)\n",
    "        arr = np.array(image, dtype=np.float32) / 255.0\n",
    "        arr = (arr.transpose(2, 0, 1) * 2.0) - 1.0\n",
    "        image_tensor = torch.from_numpy(arr)\n",
    "\n",
    "        # Tokenize with T5 (512 max length for FLUX)\n",
    "        t5_ids = self.tokenizer(\n",
    "            caption, truncation=True, padding=\"max_length\",\n",
    "            max_length=512, return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "\n",
    "        return {\"pixel_values\": image_tensor, \"t5_input_ids\": t5_ids}\n",
    "\n",
    "train_ds = CaptionImageDataset(DATASET_DIR / \"new\", tokenizer, IMG_RES)\n",
    "train_loader = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# --- Scheduler & Accelerator ---\n",
    "noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(BASE_MODEL_ID, subfolder=\"scheduler\")\n",
    "accelerator = Accelerator(mixed_precision=MIXED_PRECISION if MIXED_PRECISION in (\"fp16\",\"bf16\") else \"no\")\n",
    "device = accelerator.device\n",
    "log.info(f\"Using device: {device}\")\n",
    "\n",
    "# --- Move models ---\n",
    "vae.to(device)\n",
    "text_encoder.to(device)\n",
    "transformer.to(device)\n",
    "\n",
    "# --- Optimizer ---\n",
    "try:\n",
    "    if USE_8BIT_ADAM:\n",
    "        from bitsandbytes.optim import AdamW8bit\n",
    "        optimizer = AdamW8bit((p for p in transformer.parameters() if p.requires_grad), lr=LR)\n",
    "        log.info(\"Using 8-bit AdamW optimizer\")\n",
    "    else:\n",
    "        raise ImportError\n",
    "except Exception:\n",
    "    from torch.optim import AdamW\n",
    "    optimizer = AdamW((p for p in transformer.parameters() if p.requires_grad), lr=LR)\n",
    "    log.info(\"Using standard AdamW optimizer\")\n",
    "\n",
    "transformer, optimizer, train_loader = accelerator.prepare(transformer, optimizer, train_loader)\n",
    "transformer.train()\n",
    "\n",
    "# --- Training Loop ---\n",
    "log.info(\"Starting training...\")\n",
    "global_step = 0\n",
    "for epoch in range(999999):\n",
    "    for batch in train_loader:\n",
    "        if global_step >= MAX_STEPS:\n",
    "            break\n",
    "\n",
    "        with accelerator.accumulate(transformer):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            t5_input_ids = batch[\"t5_input_ids\"].to(device)\n",
    "\n",
    "            # Encode image → latents\n",
    "            latents = vae.encode(pixel_values).latent_dist.sample() * vae.config.scaling_factor\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.size(0),), device=device)\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Encode caption with T5\n",
    "            encoder_hidden_states = text_encoder(t5_input_ids).last_hidden_state\n",
    "\n",
    "            # Forward through transformer\n",
    "            model_pred = transformer(\n",
    "                hidden_states=noisy_latents,\n",
    "                timestep=timesteps,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "            ).sample\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(model_pred, noise, reduction=\"mean\")\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.is_main_process and global_step % 50 == 0:\n",
    "            log.info(f\"Step {global_step} | Loss {loss.item():.4f}\")\n",
    "        global_step += 1\n",
    "\n",
    "    if global_step >= MAX_STEPS:\n",
    "        break\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    transformer.save_pretrained(str(LORA_OUTPUT_DIR))\n",
    "    log.info(f\"Saved LoRA weights to {LORA_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b2e3b",
   "metadata": {},
   "source": [
    "# Image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- Load FLUX pipeline ---\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if MIXED_PRECISION == \"bf16\" else torch.float32,\n",
    ").to(accelerator.device)\n",
    "\n",
    "# --- Load trained LoRA weights into the pipeline transformer ---\n",
    "pipe.transformer = PeftModel.from_pretrained(pipe.transformer, LORA_OUTPUT_DIR)\n",
    "\n",
    "# --- Canonical style prompt ---\n",
    "CANONICAL_STYLE_PROMPT = (\n",
    "    \"food photography, professional culinary styling, top-down composition, \"\n",
    "    \"minimal background, soft natural lighting, vibrant colors, \"\n",
    "    \"high quality, detailed textures, appetizing presentation\"\n",
    ")\n",
    "\n",
    "# --- Output configuration ---\n",
    "OUT_DIR = (WORK_DIR / \"outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "guidance_scale = 3.5          # FLUX prefers lower guidance than SD1.x/2.x\n",
    "num_inference_steps = 28      # Typical sweet spot for FLUX\n",
    "height, width = 1024, 1024    # Keep resolution consistent with training\n",
    "\n",
    "# --- Prompts to test the style LoRA ---\n",
    "sample_prompts = [\n",
    "    \"delicious pasta dish with herbs and parmesan cheese\",\n",
    "    \"fresh salad with colorful vegetables and dressing\",\n",
    "    \"grilled chicken with roasted vegetables\",\n",
    "    \"chocolate dessert with berries and cream\"\n",
    "]\n",
    "\n",
    "# --- Generate samples ---\n",
    "for i, base_prompt in enumerate(sample_prompts, start=1):\n",
    "    full_prompt = f\"{base_prompt}, {CANONICAL_STYLE_PROMPT}\"\n",
    "    image = pipe(\n",
    "        prompt=full_prompt,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        height=height,\n",
    "        width=width,\n",
    "    ).images[0]\n",
    "\n",
    "    out_path = OUT_DIR / f\"flux_generated_{i:02d}.jpg\"\n",
    "    image.save(out_path, \"JPEG\", quality=95, optimize=True)\n",
    "    print(f\"✅ Saved {out_path.name} | Prompt: {full_prompt}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
